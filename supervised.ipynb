{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos una búsqueda aleatorio de los mejores hiperparámetros, esto es debido a que una búsqueda más exhaustiva para un dataset tan grande es demasiado ineficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresionLogistica(X, y, n, v_cruzada, seed):\n",
    "    log_reg = LogisticRegression(max_iter=500)\n",
    "    parametros_log_reg = {'C': np.logspace(-4, 4, 20), 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
    "    busqueda_log_reg = RandomizedSearchCV(estimator=log_reg, param_distributions=parametros_log_reg, n_iter=n, \\\n",
    "                                      scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_log_reg.fit(X, y)\n",
    "    parametros = busqueda_log_reg.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_log_reg.best_score_}')\n",
    "\n",
    "    modelo = LogisticRegression(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def SGDC(X, y, n, v_cruzada, seed):\n",
    "    sgd = SGDClassifier()\n",
    "    parametros_sgdc = {'loss': ['hinge', 'squared_error', 'log_loss'], 'alpha': np.logspace(-5, 0, 10),\\\n",
    "                       'max_iter': [100, 200], 'tol': [1e-3, 1e-4]}\n",
    "    busqueda_sgdc = RandomizedSearchCV(estimator=sgd, param_distributions=parametros_sgdc, n_iter=n,\\\n",
    "                                       scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_sgdc.fit(X, y)\n",
    "    parametros = busqueda_sgdc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_sgdc.best_score_}')\n",
    "\n",
    "    modelo = SGDClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def arbolDecision(X, y, n, v_cruzada, seed):\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    parametros_dtc = {'max_depth': list(np.arange(20, 50, 2)), 'min_samples_split': np.arange(10, 20, 2), 'criterion': ['gini', 'entropy']}\n",
    "    busqueda_dtc = RandomizedSearchCV(estimator=dtc, param_distributions=parametros_dtc, n_iter=n, scoring='f1_weighted',\\\n",
    "                                      cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_dtc.fit(X, y)\n",
    "    parametros = busqueda_dtc.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_dtc.best_score_}')\n",
    "\n",
    "    modelo = DecisionTreeClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def bosqueAleatorio(X, y, n, v_cruzada, seed):\n",
    "    rf = RandomForestClassifier()\n",
    "    parametros_rf = {'n_estimators': np.arange(100, 200, 20), 'max_features': ['sqrt', 'log2']+list(range(1, 10, 3)), \\\n",
    "                     'max_depth': list(np.arange(5, 20, 5)), 'min_samples_split': np.arange(5, 20, 5)}\n",
    "    busqueda_rf = RandomizedSearchCV(estimator=rf, param_distributions=parametros_rf, n_iter=n, scoring='f1_weighted',\\\n",
    "                                   cv = v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_rf.fit(X, y)\n",
    "    parametros = busqueda_rf.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_rf.best_score_}')\n",
    "\n",
    "    modelo = RandomForestClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "def KNN(X, y, n, v_cruzada, seed):\n",
    "    knn = KNeighborsClassifier()\n",
    "    parametros_knn = {'n_neighbors': np.arange(1, 15, 3), 'weights': ['uniform', 'distance'],\\\n",
    "                      'metric': ['euclidean', 'manhattan']}\n",
    "    busqueda_knn = RandomizedSearchCV(estimator=knn, param_distributions=parametros_knn, n_iter=n,\\\n",
    "                                    scoring='f1_weighted', cv=v_cruzada, verbose=1, random_state=seed, n_jobs=-1)\n",
    "    \n",
    "    busqueda_knn.fit(X, y)\n",
    "    parametros = busqueda_knn.best_params_\n",
    "    print(f'Los mejores parámetros son: {parametros} con una puntuación de {busqueda_knn.best_score_}')\n",
    "\n",
    "    modelo = KNeighborsClassifier(**parametros)\n",
    "    scores = cross_val_score(modelo, X, y, cv=v_cruzada, scoring='f1_weighted')\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cruzada = 5\n",
    "seed = 42\n",
    "n = 30\n",
    "\n",
    "df1_over = pd.read_csv('df tipo 1\\df1_over.csv')\n",
    "df1 = pd.read_csv('df tipo 1\\df1.csv')\n",
    "\n",
    "X, y = df1.drop('label', axis=1), df1['label']\n",
    "X_over, y_over = df1_over.drop('label', axis=1), df1_over['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'solver': 'liblinear', 'penalty': 'l1', 'C': 29.763514416313132} con una puntuación de 0.8781402610628779\n",
      "0.8780411178157668\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'tol': 0.001, 'max_iter': 100, 'loss': 'log_loss', 'alpha': 0.0004641588833612782} con una puntuación de 0.6300925051388532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5254557450526234\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Los mejores parámetros son: {'min_samples_split': 12, 'max_depth': 20, 'criterion': 'entropy'} con una puntuación de 0.8308873862182546\n",
      "0.830266655216404\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "40 fits failed out of a total of 150.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "19 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "21 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan 0.87672455 0.87984121 0.8642     0.86162288 0.87671718\n",
      "        nan 0.87983155 0.87902926 0.87956241 0.86177707 0.86178188\n",
      " 0.86456089        nan 0.8804392  0.8613809         nan        nan\n",
      " 0.87566017 0.87572031 0.87555601 0.86404939 0.86389203 0.86451953\n",
      "        nan 0.88017013        nan        nan 0.87951363 0.87537775]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los mejores parámetros son: {'n_estimators': 180, 'min_samples_split': 15, 'max_features': 'sqrt', 'max_depth': 15} con una puntuación de 0.8804392044680981\n",
      "0.8801507147119816\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teng Xin Lin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 20 is smaller than n_iter=30. Running 20 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(regresionLogistica(X, y, n, v_cruzada, seed))\n",
    "print(SGDC(X, y, n, v_cruzada, seed))\n",
    "print(arbolDecision(X, y, n, v_cruzada, seed))\n",
    "print(bosqueAleatorio(X, y, n, v_cruzada, seed))\n",
    "print(KNN(X, y, n, v_cruzada, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regresionLogistica(X_over, y_over, n, v_cruzada, seed))\n",
    "print(SGDC(X_over, y_over, n, v_cruzada, seed))\n",
    "print(arbolDecision(X_over, y_over, n, v_cruzada, seed))\n",
    "print(bosqueAleatorio(X_over, y_over, n, v_cruzada, seed))\n",
    "print(KNN(X_over, y_over, n, v_cruzada, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
